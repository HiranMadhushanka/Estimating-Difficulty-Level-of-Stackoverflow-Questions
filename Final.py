# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HKjf7ZcBBkkGY3kSCX9j3lYDfuXJHq9F

**1. Selecting Dataset**

---
"""

#Mount Google drive
from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# #change directory
# %%capture
# %cd /content/gdrive/MyDrive/FYP - Team Nova

# Commented out IPython magic to ensure Python compatibility.
#list files in current directory
# %ls

"""**2. Importing Libraries**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler,MinMaxScaler
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
import xml.etree.ElementTree as ET
import csv
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
import matplotlib.cm as cm
import pickle

#downloading dataset into google drive
# !wget https://www.ics.uci.edu/~duboisc/stackoverflow/answers.csv

#read dataset into pandas dataframe
dataset = pd.read_csv('answers.csv').drop(['Unnamed: 0'],axis=1)
dataset

"""**3. Data Preprocessig**

---


"""

#rename column headers
dataset = dataset.rename(columns={
                 'qid': 'QuestionId',
                 'aid' : 'AnswerId',
                 'i' : 'QuestionerId',
                 'j' : 'AnswererId',
                 'qt' : 'TimeOfQuestion',
                 'qs' : 'ScoreOfQuestion',
                 'tags': 'Tags',
                 'qvc': 'ViewCount',
                 'qac': 'NoOfAnswers',
                 'as' : 'ScoreOfAnswer',
                 'at' : 'TimeOfAnswer'
                 })

#show dataset
dataset

#remove unwanted columns
dataset = dataset.drop(columns=['QuestionerId','TimeOfQuestion','AnswererId','AnswerId','Tags'])
dataset.head()

#find time and score of first answer by group by QuestionId
grouped_df = dataset.groupby('QuestionId')
minimums = grouped_df.min();
dataset = minimums.reset_index()
dataset.head()

#Reanme column headers of time and score of first answer
dataset = dataset.rename(columns={
                 'TimeOfAnswer' : 'FirstResponseTimeToTheQuestion',
                 'ScoreOfAnswer' : 'ScoreOfTheFirstAnswer'
                 })
dataset.head()

#feature list
features = ['ScoreOfQuestion','ViewCount','NoOfAnswers','ScoreOfTheFirstAnswer','FirstResponseTimeToTheQuestion']

#get dataset only with features
dataset = dataset[features]

#show dataset
dataset.head()

"""3.2 Handling Missing values

---


"""

#check for null values
null_columns = dataset.columns[dataset.isnull().any()]
sum(dataset[null_columns].isnull().sum())

#check for zero values
sum((dataset[["ViewCount","NoOfAnswers","FirstResponseTimeToTheQuestion"]]==0).sum())

#round first response time for the question
dataset['FirstResponseTimeToTheQuestion'] = dataset['FirstResponseTimeToTheQuestion'].div(10000000).round(2)
dataset.head()

#show dataset
dataset.head()

"""Studying about the data distribution for extracted features

---


"""

#score of question
x = dataset['ScoreOfQuestion']
ax = sns.distplot(x)

#view count
x = dataset['ViewCount']
ax = sns.distplot(x)

#no of answers
x = dataset['NoOfAnswers']
ax = sns.distplot(x)

#score of first answer
x = dataset['ScoreOfTheFirstAnswer']
ax = sns.distplot(x)

#first response time to the question
x = dataset['FirstResponseTimeToTheQuestion']
ax = sns.distplot(x)

"""**4. Extract Proportionately proportional, Inversely proportional features**

---


"""

#find Proportionately_proportional and Inversely_proportional features
dataset['Proportionately_proportional']=dataset['ScoreOfQuestion']+ 2*dataset['FirstResponseTimeToTheQuestion']
dataset['Inversely_proportional']=2*dataset['NoOfAnswers']+dataset['ScoreOfTheFirstAnswer']+3*dataset['ViewCount']
dataset.head()

#get final feature dataset
dataset = dataset[['Proportionately_proportional','Inversely_proportional']]

"""**5. Devide dataset into training and test sets**

---


"""

#devide dataset into test and train datasets
x_train, x_test= train_test_split(dataset,test_size=0.2,random_state=0)

#show x_train
x_train

#show x_test
x_test

#get x_test as array
X_TEST = x_test.values
X_TEST

#get x_test as array
X_TRAIN = x_train.values
X_TRAIN

"""**6. Define kmeans clustering model**

---


"""

#define k_means clustering model for 5 clusters
def k_means(feature_matrix, num_clusters=5):
    km = KMeans(n_clusters=num_clusters,
                max_iter=10)
    km.fit(feature_matrix)
    clusters = km.labels_
    return km, clusters

"""**7. Finding the optimal number of clusters**

---

**7.1 Elbow Method**

---
"""

distortions = []
k_means_models = []
for i in range(2, 9):
  km_obj, clusters = k_means(feature_matrix=X_TRAIN,num_clusters=i)
  distortions.append(km_obj.inertia_)
  k_means_models.append(km_obj)

plt.plot(range(2, 9), distortions, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Distortion')
plt.show()

"""**7.2 Average silhouette method**

---


"""

silhouette_scores = [silhouette_score(X_TRAIN ,model.labels_) for model in k_means_models]
_ = plt.plot(range(2,9), silhouette_scores, "bo-", color='blue',linewidth=3,markersize=8)

range_n_clusters = [2, 3, 4, 5, 6]
 
for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)
 
    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X_TRAIN) + (n_clusters + 1) * 10])
 
    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X_TRAIN)
 
    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X_TRAIN, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)
 
    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X_TRAIN, cluster_labels)
 
    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]
 
        ith_cluster_silhouette_values.sort()
 
        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i
 
        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)
 
        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
 
        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples
 
    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")
 
    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")
 
    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
 
    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X_TRAIN[:, 0], X_TRAIN[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')
 
    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')
 
    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')
 
    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")
 
    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')
 
plt.show()

plt.scatter(X_TRAIN[:, 0], X_TRAIN[:, 1], s=20);
plt.show()

"""**8. Training the K-means algorithm on the training dataset**

---


"""

kmeans_model = KMeans(n_clusters = 3,  init= 'k-means++', max_iter= 100000).fit(X_TRAIN)
labels = kmeans_model.labels_
y_kmeans = kmeans_model.predict(X_TRAIN)
x_train['Cluster'] = labels
def ClusterLabel(x,y):
  if x == 0:
      return "Basic"
  elif x == 1:
      return "Advanced"
  elif x == 2:
      return "Intermediate"   
x_train['Cluster_Label'] = x_train['Cluster'].apply(ClusterLabel, args=('Cluster',))
x_train

#visualization of the clustered data
plt.scatter(X_TRAIN[:, 0], X_TRAIN[:, 1], c=y_kmeans, s=20, cmap='summer')
centers = kmeans_model.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k');
for i, c in enumerate(centers):
    plt.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')
    plt.title("The visualization of the clustered data.")
    plt.xlabel("Proportionately_proportional")
    plt.ylabel("Inversely_proportional")
plt.text(350, 55000, r'0 - Basic')
plt.text(350, 51000, r'2 - Intermediate')
plt.text(350, 47000, r'1 - Advanced')
plt.show()

"""**9. Save model and Predict difficulty level of test dataset by model**

---


"""

#save model
pickle.dump(kmeans_model, open("save.pkl", "wb"))

#load model
model_ = pickle.load(open("save.pkl", "rb"))

# Predict difficulty level of test dataset by model
t = X_TEST
v = model_.predict(t)
x_test['Cluster'] = v
def ClusterLabel(x,y):
  if x == 0:
      return "Basic"
  elif x == 1:
      return "Advanced"
  elif x == 2:
      return "Intermediate"   
x_test['Cluster_Label'] = x_test['Cluster'].apply(ClusterLabel, args=('Cluster',))
x_test

"""**10. Cluster Evaluation**

---


"""

#Evaluation cluster by silhouette score
silhouette_score(X_TRAIN, labels, metric = 'euclidean')

"""**11. Creating Api for model**

---


"""

#install fastapi library
!pip install fastapi

#create question model
from pydantic import BaseModel

class Question(BaseModel):
    ScoreOfQuestion: float 
    ViewCount: float 
    NoOfAnswers: float 
    ScoreOfTheFirstAnswer: float
    FirstResponseTimeToTheQuestion: float 
    class Config:
        schema_extra = {
            "example": {
                "ScoreOfQuestion": 20, 
                "ViewCount": 1000,
                "NoOfAnswers": 6, 
                "ScoreOfTheFirstAnswer":50,             
                "FirstResponseTimeToTheQuestion": 1230000
            }
        }

#create app
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import pickle

app = FastAPI()

origins = [

    "http://localhost.tiangolo.com",

    "https://localhost.tiangolo.com",

    "http://localhost:4200",

    "http://localhost:8080",

]

app.add_middleware(

    CORSMiddleware,

    allow_origins=origins,

    allow_credentials=True,

    allow_methods=["*"],

    allow_headers=["*"],

)

@app.on_event("startup")
def load_model():
    global model
    model = pickle.load(open("save.pkl", "rb"))

@app.get('/')
def index():
    return {'message': 'This is the homepage of the API '}

@app.post('/predict')
def get_question_difficulty(data: Question):
    received = data.dict()

    ScoreOfQuestion = received['ScoreOfQuestion']
    ViewCount = received['ViewCount']
    NoOfAnswers = received['NoOfAnswers']
    ScoreOfTheFirstAnswer = received['ScoreOfTheFirstAnswer']
    FirstResponseTimeToTheQuestion = received['FirstResponseTimeToTheQuestion']/10000000

    Proportionately_proportional=ScoreOfQuestion+2*FirstResponseTimeToTheQuestion
    Inversely_proportional=ScoreOfTheFirstAnswer+2*NoOfAnswers+3*ViewCount

    pred_difficulty_level = model.predict([[Proportionately_proportional, Inversely_proportional]]).tolist()[0]
    if pred_difficulty_level == 0:
      return {'difficulty_level': 'Basic'}
    elif pred_difficulty_level == 1:
      return {'difficulty_level': 'Advanced'}
    else:
      return {'difficulty_level': 'Intermediate'}

#upgrade notebook
!pip install --upgrade notebook

#install colab code
!pip install colabcode

#create server
from colabcode import ColabCode
server = ColabCode(port=10000, code=False)

#run server
server.run_app(app=app)